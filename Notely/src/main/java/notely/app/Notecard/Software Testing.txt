Software Testing
Software
Acceptance Test Certificate@a document (typically one-page long), which is used to formally record the successful completion of the acceptance test for a particular application under test (AUT). The certificate is signed by a senior representative of the organization accepting the AUT and the organization delivering the AUT.
Acceptance Testing@Formal testing conducted to enable a user, customer, or other Authorized entity to determine whether to accept a system or component. This testing phase is used to test the system as a whole to ensure that it meets its stated business requirements and that it performs within defined constraints. This testing phase takes place after system testing has been completed (also see user acceptance testing and operations acceptance testing).
AUT (Application Under Test)@The application or program that is the focus of the testing process. Application under test is abbreviated as AUT, and is synonymous with system under test (SUT) in this document. SUT (System Under Test): System under test is abbreviated as SUT, and is synonymous with application under test (AUT) in this document.
Automated Testing@The use of software to control the execution of tests, the com¬parison of their observed results to predicted results, the setting up of test pre¬conditions, and other test control and test reporting functions.
Automated Testing Tools@Software tools that Automate the testing process in order to reduce effort, timescales, and cost in the testing of software.
Automated Test Script@A series of instructions in a computer readable form, which allows an Automated testing tool to reproduce the navigation and verification points programmed into the tool to perform testing of the AUT. An Automated test script is analogous to a manual test script, with verification points analogous to manual test cases.
Black Box Testing@Test case selection that is based on an analysis of the specifica¬tion of the component without reference to its internal workings. That is, testing where the test cases are designed without any knowledge of how the AUT is con¬structed. The test cases are derived from a knowledge of the AUT's external behavior only.
Boundary Value@An input value or output value that is on the boundary between equivalence classes, or at an incremental distance either side of the value.
Boundary Value Analysis@A test case design technique for a component in which test cases are designed that include representatives of boundary values.
Build Verification/Smoke Testing@A testing technique that verifies the navigational struc¬ture of the AUT. Typically performed depth first, each menu and its component menu items (and submenus) are invoked to verify that they run some specific aspect of the AUT's functionality or open a dialog box. In the latter case, further navigation is performed to verify that dialog box buttons invoke AUT functionality or open further dialog boxes.
Business Process Testing@A testing technique that verifies a typical business use of the AUT. Business process testing takes a typical scenario involving user interaction with the AUT and implements this as a test. Use cases are often cited as being a particularly good source of business process tests.
Business Scenario/Thread@The combination of navigation through the AUT and the invoca¬tion of specific functionality to support a particular business use of the AUT. For example, in a banking application, the collection of steps involved in creating a new account could be considered a business thread.
Commercial Off-the-Shelf (COTS)@Commercially available software products/tools that can be purchased from vendors.
Compatibility/Interoperability Testing@Compatibility/Interoperability testing verifies that when the AUT runs in the live environment its operation does not impact adversely on other systems and vice versa
Crash/Skim Testing@A testing technique used to determine the fitness of a new build or release of an AUT to undergo more thorough testing. Crash testing seeks to discover if the AUT is reliable and robust, or if it "crashes" during routine use. In essence, crash testing is a "pre-test" activity that could form one of the acceptance criteria for receiving the AUT for testing.
Development/Testing Environment@The IT environment (including the hardware, software, data, and support infrastructure) used by the development team to support the development of the AUT. Typically, the early phases of testing (such as unit testing) are conducted on the development environment. Also see live environment and test environment.
Development Team@The project team with responsibility for developing the AUT.
Development Team Leader@The member of staff responsible for leading the team developing the AUT. Liaises with the test team leader to report on the progress of the development of the AUT and likely dates for delivery of AUT for testing purposes.
End Users@The members of staff who will use the delivered AUT in support of their work
Equivalence/Partition Class@A set of the component's input or output domains for which the component's behavior is assumed to be the same from the component's specification. An equivalence class may also be termed an equivalence partition.
Equivalence Class Testing@A test case design technique for a component in which test cases are designed to execute representatives from equivalence classes. This form of testing may also be termed domain or partition testing.
Error Guessing/ Experienced Based Testing@A test case design technique where the experience of the tester is used to postulate what faults might be present in a component or AUT, and to design tests specifically to expose them.
Fault Recovery Testing@This form of testing verifies that following an error or exceptions, the AUT is restored to a "normal" state, such as the initial state of the AUT when it is first executed. This form of testing may also be used to verify the successful rollback and recovery of the data used or manipulated by the AUT.
Functional Requirements@Traditionally, those requirements for the AUT that have been specified within the functional specification document for the AUT. Also see nonfunctional requirements.
Functional Specification@A document that describes in detail the characteristics of the AUT with regard to its intended capability.
Functional Testing@The process of testing the AUT to verify that it meets its functional requirements. These requirements are typically recorded in a requirements specification document.
Incremental Testing@A testing technique used in integration testing, where untested modules are combined with the module under test incrementally, with the func¬tionality of the module (plus the added modules) being tested (retested) as each module is added.
Independent Test Observer@A member of staff, who is independent from the devel¬opment and testing teams, who observes the testing process and is involved in signing off the test result record forms for each test case.
Integration/Module/Link Testing@Testing performed to expose faults in the interfaces and in the interaction between integrated components. This testing phase is used to identify problems with communications between modules or units, resource contention, and to verify the system design. This testing phase takes place after unit testing but before system testing and may also be termed link or module testing.
IT Systems Administrator/ IT Systems Manager@A member IT staff within an organization with respon¬sibility for administering the corporate IT facilities and for performing the day-to- day operations of the system (such as installation of software, upgrade of operating system software, back-up and archive, and preventive maintenance).
Live Data@The actual data used by an organization to support its business activities. This is distinct from any test data that may have been generated to represent the live data for the purposes of testing.
Live Environment@The actual IT environment (including the hardware, software, data, and support infrastructure) used by an organization to support its business activities. This is distinct from any testing environment that may have been set up to represent the live environment for the purposes of testing. Also see development environment and test environment.
Load Testing@A nonfunctional testing technique that determines the AUT's ability to cope with sudden instantaneous peaks loads (such as the sudden attempt by all users of a system to log on simultaneously) (as opposed to the sample oper¬ating conditions likely to have been used during development of the AUT).
Negative Testing@Testing aimed at demonstrating that the software does not behave in a manner prohibited in its specification.
Nonfunctional Requirements@Within the context of testing, the term nonfunctional requirements refer to the testing of those aspects of the AUT not specified within the functional specification document for the AUT. For example, performance, volume, stress, and usability testing are examples of nonfunctional testing
Nonfunctional Testing@The testing of those requirements that do not relate to func¬tionality. Literally, the testing of those aspects of the AUT that are not explicitly specified in the functional specification document (although in practice, issues of performance and usability may be well defined in exemplary examples of such documents). Nonfunctional testing includes: performance, stress, volume, and usability testing.
Operations Acceptance Testing@This is a form of acceptance testing conducted from the operations perspective (i.e., involving representatives from the operations staff). (Also see acceptance testing.)
Operations Representative@A nominated member of the operations community for the ALT who will be expected to participate in or to nominate other operations staff to participate in the operations acceptance testing of the system (it is important that the operations representative be a genuine user of the system, and not, for example, a manager who manages operations users of the system).
Operations Users@The members of staff who will execute the administrative aspects of the delivered software system in support of the end users. For example, typical tasks performed by the operations users include setting up user access and privileges, back-up and archive of system data, and system recovery.
Pairwise/All-Pairs Testing@A test case design technique that seeks to select a subset of all possible pairs of parameters and combinations of values, to provide the most effective test coverage, and in combination with the other test design will provide good test results.
Performance Testing@Testing conducted to evaluate the compliance of a component or AUT with specified performance requirements. Performance testing is a particular type of nonfunctional testing that measures the speed of execution and response of the AUT under typical working conditions with the intention of determining that these characteristics will meet the user's requirements (expectations) for the AUT.
Pseudocode@A form of structured English used by programmers in developing the program code to represent the functionality, structure, and flow of the application. Frequently used in static testing techniques (such as code review or walkthrough).
QA@An acronym for the term quality assurance.
RAD (Rapid Application Development)@A software development approach that emphasizes the rapid delivery of the software under development. Such approaches typically engage the end users in the development process as early as possible by the use of prototypes and mock-ups of the system, and employ stringent planning and man¬agement using an iterative development style with strictly observed milestones and budgets.
Regression Testing@A testing technique in which the AUT is tested following mod¬ifications or extensions to ensure that the overall functionality of the system has not been compromised. Regression testing may also take place following changes to the environment in which the system is resident (e.g., changes to the operating system or to hardware).
Reliability Testing@A nonfunctional testing requirement that verifies that the AUT is robust and reliable in normal use, ensuring for example, that the AUT does not suffer from catastrophic failures or from memory leak problems.
Reuse Packs@These are documents that allow testers to quickly and easily rerun some or all the tests performed during an earlier testing phase. Where modifications have been made to a system, reuse packs can speed up the process of regression testing. Similarly, reuse packs can enable selected tests to be rerun in later testing phases should they be required.
Safety Testing@A testing technique intended to validate that the AUT or SUT meets its requirements in terms of safety of operation. This technique is particularly appropriate for the testing of safety critical systems.
Security Testing@Security testing is focused on ensuring the AUT and/or its data is accessible to only Authorized users. Security tests can be designed, implemented, and executed within any testing phase, but are typically conducted at system test.
Simulation/Test Harness@The representation of selected behavioral characteristics of one phys¬ical or abstract system by another system. In testing terms, that is the use of a computer program to represent elements of the testing environment that are not available during testing. For example, if the AUT is being developed and tested on a standalone workstation but will be delivered on a networked system, it may be necessary to simulate typical network traffic during testing.
Simulator@A software system or computer program responsible for providing simu¬lation services in support of the testing of an AUT.
SRS (Software Requirements Specification)@A document that records the requirements for a par-ticular application or software system. These are traditionally obtained from the intended user(s) of the system by an analyst using some form of requirements capture method (such as use case modeling).
State Transition Analysis/State Transition Testing@A test case design technique in which the analysis of the various states the. AUT can be in, and the transitions between those states supports the creation of a series of both positive and negative tests.
Static Testing@Static testing deals with the inspection of the AUT in isolation (i.e., not in a running state). Static testing typically takes place early in the development of the AUT and involves techniques such as code review and code walkthrough.
Stress Testing@Testing conducted to evaluate a system or component at or beyond the limits of its specified requirements. That is, a particular type of nonfunctional testing that examines the ability of the AUT to cope with instantaneous peak loads with the aim of identifying defects that only appear under such adverse conditions.
Systems Integration Testing@An optional testing phase (typically for those software systems that have a significant requirement to work or interoperate with other software systems) that is used to ensure that the AUT can interoperate successfully with any other software systems with which it needs to communicate, and to ensure that the AUT is compatible with other systems that may be running in the same environment. Systems integration testing takes place before acceptance testing and after system testing (under certain circumstance, system integration and system testing may be performed together).
System Testing@The process of testing an integrated system to verify that it meets specified requirements. That is, a testing phase that is used to test the AUT as a complete system to identify any functional problems not found in unit testing, assess nonfunctional requirements such as performance and usability, and estab¬lish confidence that the system will be accepted by the users. System testing takes place after integration testing and before acceptance testing.
Test Analyst@A member of the testing team responsible for inspecting the requirements document for the AUT, analyzing the test requirements, designing the test cases, and drafting the test scripts (see Appendix A for complete TORs of the members of staff involved in the testing process).
Test Case@A specific test intended to verify one particular aspect or requirement of the AUT, and which will contain a set of input values, execution preconditions, predicted outcome and objective for the test. A test script may contain one or more test cases.
Test Data@The data used during testing to exercise the AUT. To provide complete confidence in the results of testing, the test data should be live data, however, there are many reasons why it may not be appropriate to use live data (e.g., where the data contains confidential data or information relating to security issues, or where the testing may threaten the integrity of the live data). Under these circumstances, it may be possible to use a copy of the live data. If neither of these options is available, it may be necessary to generate handcrafted data. Handcrafted data can also be introduced into the live or copied data in order to explicitly test some boundary or error condition in the AUT. Also see live data.
Tester@A member of the testing team responsible for conducting testing of the AUT by following a test script. (See Appendix A for complete TORs of the members of staff involved in the testing process).
Test Error@A specific test result category. The test error result indicates that the associated test case failed, but that the AUT performed correctly, that is, the test case itself was in error (e.g., a typographical error in the test case).
Testing@The process consisting of all lifecycle activities, both static and dynamic, concerned with planning, preparation, and evaluation of software products and related work products to determine that they satisfy specified requirements, to demonstrate that they are fit for purpose, and to detect defects.
Testing Manager@A member of staff responsible for managing the entire testing process within an organization.
Testing Programmer@The complete organizational specification for an organization engaged in performing formal, rigorous testing of software systems.
Testing Project@A project whose goal is to verify the suitability of the AUT for its intended purpose.
Test/Testing Team@A dedicated team responsible for performing the higher phases of testing of the AUT (that is, system testing and above). The testing team will be managed by a test team leader and will contain one or more test analysts and one or more testers.
Test Log@A chronological record of all relevant details of a testing activity.
Test Plan Document@One of the artifacts generated by the test team leader during a testing project. The test plan document will be used as the basis of project management control throughout the testing process, and contains information specifying the approach to testing and any constraints, risks, and dependencies, the test assumptions and exclusions, test entry and exit criteria, project controls, and a plan for completing the testing (including contingency for any retest activities). Appendix C contains a test plan document template.
Test Repository@The means of organizing all of the testing assets (such as test cases, test logs) employed by a project using an Automated testing tool during a testing project.
Test Result Category@A member of a set of results that can be applied to the outcome of a particular test case. For example, typical test result categories include:
a) Pass - the observed test result conforms to the expected result
b) Test Error - the observed result is correct, but the expected result is incorrect
c) Acceptable - the observed result indicates that the AUT differs from the agreed specification but is acceptable, requiring no change to the system but a change to the functional specification
d) Tolerable - the observed result is incorrect, the AUT is workable and shall be accepted, but the fault must be rectified within an agreed time.
e) Intolerable - the observed result is incorrect and the fault must be corrected before the AUT passes this testing phase.
Test Result Record@A form that is used to record the results of executing a particular test case. A template copy of a test result record form can be found in Appendix F.
Test Rig@A flexible combination of hardware, software, data, and interconnectivity, which can be configured by the test team to simulate a variety of different live environments on which an AUT can be delivered.
Test Script@A prescriptive document describing in detail how a test is to be conducted, the inputs, and the expected behavior of the item being tested. A test script will contain a number of test cases. In Automated testing terms, a test script is an Automated test procedure used with a test harness or tool.
Test Specification Document@One of the artifacts generated by the test team leader during a testing project. The test specification document provides the specification for the testing process required for formal verification of the AUT. Appendix D contains a test specification document template.
Test Team Leader@A member of the testing team responsible for managing the day- to-day tasks of the testing team.
Thread Testing@A testing technique used to test the business functionality or business logic of the AUT in an end-to-end manner, in much the same way a user or an operator might interact with the system during its normal use.
Tool Support@The use of an Automated testing tool to support the testing process in order to reduce effort, timescales, and cost in software testing.
TOR@Terms of reference (Terms of Reference for Testing Staff).
Usability Testing@A particular type of nonfunctional testing that focuses on the ease of use of the AUT from the perspective of the end user. Issues to be considered in usability testing include the intuitiveness, consistency, and clarity or understand- ability of the user interface. See also performance, stress, and volume testing.
User Acceptance Testing@This is a form of acceptance testing conducted from the user perspective (i.e., involving user representatives). (Also see acceptance testing.)
User Representative@A nominated member of the user community for the AUT who will be expected to participate in or to nominate other users to participate in user acceptance testing of the system (it is important that the user representative be a genuine user of the system, and not, for example, a manager who manages users of the system).
V&V@An abbreviation of Verification and Validation. This is the process of confirming that the AUT meets its requirements and has been developed using best practice/formal process. Validation "Are we building the product right?" The process by which it is confirmed that the AUT has been developed in line with best practice and applicable standards. The International Software Testing Qualifications Board (ISQTB - [72]) definition of validation is as follows. Confirmation by examination and through provision of objective evidence that the requirements for a specific intended use or application have been fulfilled. Verification "Are we building the right product?" The process by which it is confirmed that the AUT meets its requirements.
Verification Point@A verification point is an element of an Automated test script (i.e., a script that can be run using an Automated testing tool), which verifies some aspect of the functionality of the AUT. A verification point in an Automated test script is analogous to a test case in a manual test script.
Volume Testing@A nonfunctional testing technique that determines the AUT's ability to cope with large volumes of data (as opposed to the sample data likely to have been used during development of the AUT).
Waterfall Development@A particular approach to software development that views the process as a series of linear steps each of which (in the strictest sense of the term) must be completed before the following step can begin. The steps traditionally include requirements, specification, design, and implementation.
White/Glass Box Testing@Testing where the test cases are designed with the knowledge of how the AUT is constructed.
